{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgQOnFaMafaj"
   },
   "source": [
    "<div id=\"header\" align=\"center\">\n",
    "    <h1>Rapport S6-APP4</h1>\n",
    "    <br />\n",
    "    <h2>GRO620 | Vision par ordinateur</h2>\n",
    "    <br />\n",
    "    <h3>Gabriel Cabana | cabg2101</h3>\n",
    "    <h3>Olivier Roy | royo2206</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1622860376049,
     "user": {
      "displayName": "Olivier Roy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjv0vxfjJVu_NgiwC4waqnVdwTo-bHKd81MIH8Rl4Q=s64",
      "userId": "10268384989169542885"
     },
     "user_tz": 240
    },
    "id": "-VU4KSYoafal",
    "outputId": "b102708c-5370-4269-b5b3-046d8232bbce"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rng\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"GRO620 - Problématique\")\n",
    "print(\"OpenCV version\", cv2.__version__)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1622860376188,
     "user": {
      "displayName": "Olivier Roy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjv0vxfjJVu_NgiwC4waqnVdwTo-bHKd81MIH8Rl4Q=s64",
      "userId": "10268384989169542885"
     },
     "user_tz": 240
    },
    "id": "nTsbHtmDbQ7Y",
    "outputId": "2461e3fd-7391-41f4-8603-0c8dd9dad275"
   },
   "outputs": [],
   "source": [
    "## Si vous utilisez Google Colab, vous devez d'abord monter votre Google Drive\n",
    "## où se trouve vos données. \n",
    "## Commentez les trois lignes suivantes en ajustant le chemin vers votre propre\n",
    "## dossier :\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#%cd /content/gdrive/MyDrive/BACC/S6/APP4\n",
    "\n",
    "## Pour retrouver le chemin depuis Jupyter, vous pouvez utiliser ceci :\n",
    "# !ls /content/gdrive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction du problème\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Pour permettre de rendre possible le recyclage de quincaillerie utilisée dans les meubles préfabriqués d'une entreprise de mobilier, une solution est proposée. Il s'agit d'un système de vision permettant de détecter, classer et localiser les vis et boulons circulant sur un convoyeur. Pour démontrer l'efficacité de cette solution, une procédure de résolution du problème est présentée pour comprendre l'origine de la chaîne de traitement d'images utilisée. Une analyse des résultats est ensuite présentée permettant de facilement interpréter les résultats du système sur les images de démonstration utilisées.\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "## 2. Procédure de résolution\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Les objets qu'il faut détecter sont des vis sur une courroie de convoyeur en caoutchouc noir texturé. Comme les vis sont des objets longs et minces, certaines particularités doivent être prises en compte pour que la détection des pièces soit possible. Également, la surface de fond étant un caoutchouc noir texturé, il faut s'assurer que le traitement d'image permette d'éliminer le plus possible le bruit causé par les réflections de lumière et par la texture de la courroie.\n",
    "</div>\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Les vis étant en acier réfléchissant, les réflections spéculaires de la source d'éclairage donne une couleur perçue par la caméra quasiment blanche par rapport au fond de l'image. Ça permet d'utiliser l'image sans faire de modifications au niveau du contraste ou de l'exposition.\n",
    "</div>\n",
    "    \n",
    "### 2.1. Détection\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Pour détecter les pièces sur le convoyeur, il faut être capable de déterminer quels pixels dans l'image appartiennent à des vis. Il faut également être capable de regrouper les pixels appartenant à une vis avec les pixels adjacents qui appartiennent également à une vis pour créer des groupes correspondants à des objets détectés. En pratique, une série de traitements d'image est utilisée pour atteindre ce but.\n",
    "</div>\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Le premier traitement qui devrait être appliqué est de convertir l'image couleur en nuances de gris. Cela permet de faire le traitement sur un seul canal de couleur et de simplifier le traitement. Comme l'arrière-plan n'est pas uniforme et cause des réflections de lumières indésirables, il est préférable de flouter l'image pour limiter le bruit. Pour séparer les pixels appartenant à l'arrière-plan des pixels appartenant aux vis, un seuillage (threshold) peut être appliqué permettant de binariser l'image. Si le seuillage est trop agressif, ce qui peut arriver lorsque les objets détectés sont assez minces, une vis pourrait séparée en deux morceaux dans l'image résultante ou carrément ignorée. Pour s'assurer que les vis soient détectées comme étant une seule entité, une transformation morphologique de dilatation peut être appliquée sur l'image. Cela permet d'enfler la surface occupée par les vis dans l'image et ainsi rejoindre les potentiels entités qui ont été séparées lors du seuillage. Un algorithme de détection de bordures dans l'image peut ensuite être appliqué pour faciliter la recherche de contours. Finalement, un algorithme de suivi de contours permet de discrétiser les différents contours dans l'image et d'obtenir une liste d'objets détectés.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<!-- For web browser in dark mode (comment otherwise) -->\n",
    "<style>\n",
    "img {\n",
    "    -webkit-filter: invert(100%);\n",
    "    filter: invert(100%);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div id=\"pipeline-diagram\" align=\"center\">\n",
    "    <img src=\"./input/pipeline_diagram_cabg2101_royo2206.svg\" alt=\"Diagramme du pipeline\" width=\"100%\"/>\n",
    "    <h2> Figure 1 ― Diagramme du traitement du pipeline. </h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Classement\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Une fois les vis détectées dans l'image, il est possible de les classer selon leur taille et de les identifier à l'aide d'un numéro. La recherche de contours ayant créé des groupes de pixels appartenant à un seul contour, il est maintenant plus facile de mesurer la taille des éléments détectés. Un numéro peut être assigné à chaque contour pour identifier l'objet. Si un rectangle est tracé en fonction des dimensions minimales d'un contour, celui-ci sera orienté dans la direction approximative de la vis. Les dimensions du rectangle représentent alors les dimensions de la vis et il est maintenant évident qu'il est possible de classer chaque vis selon sa longueur selon la taille du rectangle.\n",
    "</div>\n",
    "\n",
    "### 2.3. Localisation\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Pour localiser les différentes vis, la position sur le plan $\\left(x,y\\right)$ du convoyeur est recherchée ainsi qu'un angle entre 0 et 180°. Le rectangle de taille minimum tracé précédemment permet également d'obtenir la position de son centre et l'angle de la vis. Cependant, la position obtenue n'est pas dans le repère recherché. Il faut donc faire une projection des positions obtenus du repère de la caméra $\\{C\\}$ jusqu'au repère du convoyeur $\\{0\\}$ grâce aux informations intrinsèques et extrinsèques disponibles. Comme la matrice de transformation $T$ entre les deux repères est déjà fournie, il suffit de calculer la matrice de la caméra $P$ à partir de la matrice de calibration $K$ et de la matrice de transformation.\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Mise en oeuvre du pipeline\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Après avoir expliqué la théorie derrière les différentes étapes de traitements requis pour faire la détection, la classification et la localisation de vis dans les images, voyons comment implémenter les méthodes en Python pour faire fonctionner le système.\n",
    "</div>\n",
    "\n",
    "### 3.1. Méthodes des objets\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Différentes fonctions ont été définies pour rendre la programmation plus modulaire et permettre de la rendre plus facile à interpréter. Les méthodes ci-dessous permettent de retourner les informations nécessaires ou de faire les vérifications concernant les items détectés.\n",
    "</div>\n",
    "\n",
    "- `itemType()`:  Retourne le type de vis selon sa longueur ;\n",
    "- `itemAngle()`: Retourne l'angle de la vis en radians ;\n",
    "- `itemInfo()`:  Retourne un dictionnaire contenant les informations pertinentes ;\n",
    "- `itemMatch()`: Vérifie si un contour détecté est dans la plage de tolérance d'un contour détecté précédemment ;\n",
    "- `itemProportions()`: Vérifie si les proportions d'un contour détecté sont valides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "126NR26CM7WeuV3o-Eu8SjOn04l0OONEi"
    },
    "executionInfo": {
     "elapsed": 3401,
     "status": "ok",
     "timestamp": 1622860380559,
     "user": {
      "displayName": "Olivier Roy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjv0vxfjJVu_NgiwC4waqnVdwTo-bHKd81MIH8Rl4Q=s64",
      "userId": "10268384989169542885"
     },
     "user_tz": 240
    },
    "id": "L7AtES9Dafan",
    "outputId": "95e5cff7-e652-4095-e2d0-2b778f0719c9"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "### ITEM METHODS                               ###\n",
    "##################################################\n",
    "\n",
    "def itemType(size):\n",
    "    # Return item type depending on its longer dimension \n",
    "    item_length = np.abs(size).max()\n",
    "    \n",
    "    if item_length < 100:\n",
    "        item_type = \"courte\"\n",
    "    else:\n",
    "        item_type = \"longue\"\n",
    "    \n",
    "    return item_type\n",
    "\n",
    "def itemAngle(angle, width, height):\n",
    "    # Return item angle between 0 and pi radians.\n",
    "    if(width < height):\n",
    "        angle -= 90\n",
    "\n",
    "    return angle * -np.pi/180\n",
    "\n",
    "def itemInfo(position, size, theta, box):\n",
    "    # Return all info about an item in a dictionary\n",
    "\n",
    "    #Identify item type.\n",
    "    item_type = itemType(size)\n",
    "    \n",
    "    # Change angle into a more intuitive value.\n",
    "    theta = itemAngle(theta, size[0], size[1])\n",
    "    \n",
    "    return {\"type\": item_type, \"x\": position[0], \"y\": position[1], \"w\": size[0], \"h\": size[1], \"angle\": theta, \"box\": box}\n",
    "\n",
    "def itemMatch(items, new_item):\n",
    "    # Verify if a detected contour was already detected previously\n",
    "    known_item = False\n",
    "    pos_tol    = 10            # px\n",
    "    new_x      = new_item[\"x\"] # px\n",
    "    new_y      = new_item[\"y\"] # px\n",
    "\n",
    "    for item in items:\n",
    "        saved_x = item[\"x\"] # px\n",
    "        saved_y = item[\"y\"] # px\n",
    "\n",
    "        if abs(saved_x - new_x) < pos_tol and \\\n",
    "           abs(saved_y - new_y) < pos_tol:\n",
    "            known_item = True\n",
    "            \n",
    "    return known_item\n",
    "\n",
    "def itemProportions(size):\n",
    "    # Verify that the maximum and minimum dimensions are correct\n",
    "    width_max  = 50 # px\n",
    "    length_min = 50 # px\n",
    "    \n",
    "    if size.max() < length_min or \\\n",
    "       size.min() > width_max:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Méthodes de traitement d'image\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Les méthodes utilisées pour faire le traitement d'image regroupe les fonctions permettant d'appliquer les traitements de façon séquentielle pour obtenir l'information souhaitée, mais aussi celles qui permettent de trier et afficher les contours détectés.\n",
    "</div>\n",
    "    \n",
    "- `getImageContours()`:\n",
    "    - Conversion de BGR en nuances de gris ;\n",
    "    - Application d'un filtre gaussien pour flouter l'image ;\n",
    "    - Application d'un seuillage pour binariser l'image ;\n",
    "    - Application d'une transformation morphologique de dilatation pour amplifier les traits ;\n",
    "    - Détection des bordures ;\n",
    "    - Recherche de contours ;\n",
    "    - Si souhaité, retour d'images montrant les étapes de traitement du pipeline.\n",
    "- `sortImageContours()`:\n",
    "    - Validation des contours détectés (proportions, proximité / superposition avec un contour détecté précédemment) ;\n",
    "    - Tri des contours en fonction de leur position dans l'image.\n",
    "- `drawImageContours()`:\n",
    "    - Affichage des cadre de délimitation ainsi que des numéros d'identification sur l'image ;\n",
    "    - Si souhaité, affichage des cadre de délimitation ainsi que des numéros d'identification sur la dernière étape de traitement du pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### IMAGE METHODS                              ###\n",
    "##################################################\n",
    "\n",
    "def getImageContours(image, get_pipeline=False):\n",
    "    blur_dim      = 11\n",
    "    threshold_min = 190\n",
    "    threshold_max = 200\n",
    "    dilation_dim  = 3\n",
    "    canny_min     = 100\n",
    "    canny_max     = 150\n",
    "    \n",
    "    # Convert from BGR to grayscale.\n",
    "    img_mono = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Blur using a Gaussian Filter.\n",
    "    img_blur = cv2.GaussianBlur(img_mono, (blur_dim, blur_dim), 0)\n",
    "    \n",
    "    # Convert to binary with threshold.\n",
    "    _, img_threshold = cv2.threshold(img_blur, threshold_min, threshold_max, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Dilate features to amplify contour detection.\n",
    "    kernel = np.ones((dilation_dim, dilation_dim), np.uint8)\n",
    "    img_dilated = cv2.dilate(img_threshold, kernel, iterations = 1)\n",
    "    \n",
    "    # Detect edges using Canny.\n",
    "    img_edges = cv2.Canny(img_dilated, canny_min, canny_max)\n",
    "    \n",
    "    # Find contours.\n",
    "    contours, hierarchy = cv2.findContours(img_edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not get_pipeline:\n",
    "        return contours, hierarchy, None\n",
    "    else:\n",
    "        pipeline = list()\n",
    "        pipeline.append(img_mono)\n",
    "        pipeline.append(img_blur)\n",
    "        pipeline.append(img_threshold)\n",
    "        pipeline.append(img_dilated)\n",
    "        pipeline.append(img_edges)\n",
    "        return contours, hierarchy, pipeline\n",
    "    \n",
    "def sortImageContours(contours):\n",
    "    items = list()\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Get minimum area rectangle which stores position (x, y), size (width, height), and angle (theta)\n",
    "        new_rect     = cv2.minAreaRect(contour)\n",
    "        \n",
    "        # Split data.\n",
    "        new_position = np.asarray(new_rect[0]) # Position (x, y)\n",
    "        new_size     = np.asarray(new_rect[1]) # Size (width, height)\n",
    "        new_angle    = new_rect[2]             # Angle (theta)\n",
    "        \n",
    "        # Create item dictionary for further processing.\n",
    "        new_item     = itemInfo(new_position, new_size, new_angle, np.int0(cv2.boxPoints(new_rect)))\n",
    "\n",
    "        # If contour is new, save information.\n",
    "        if not itemMatch(items, new_item) and itemProportions(new_size):\n",
    "            items.append(new_item)\n",
    "            \n",
    "    # Sort items (up and down, left to right).\n",
    "    items = sorted(items, key = lambda k: k[\"y\"])\n",
    "    items = sorted(items, key = lambda k: k[\"x\"])\n",
    "            \n",
    "    return items\n",
    "    \n",
    "def drawImageContours(image, items, pipeline=None):\n",
    "    for i, item in enumerate(items):\n",
    "        color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "        \n",
    "        # Draw bounding box.\n",
    "        cv2.drawContours(image, [item[\"box\"]], 0, color, 2)\n",
    "        \n",
    "        # Identify bounding box.\n",
    "        cv2.putText(image, str(i), (int(item[\"x\"]), int(item[\"y\"])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "        if pipeline is not None:\n",
    "            # Draw bounding box on last pipeline step.\n",
    "            cv2.drawContours(pipeline[-1], [item[\"box\"]], 0, (128, 128, 128), 2)\n",
    "            \n",
    "            # Identify bounding box on last pipeline step.\n",
    "            cv2.putText(pipeline[-1], str(i), (int(item[\"x\"]), int(item[\"y\"])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Méthodes de transformations\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Pour convertir les positions obtenues par les contours du repère de la caméra $\\{C\\}$ dans le repère du convoyeur $\\{0\\}$, certaines transformations et opérations matricielles sont nécessaires. Les positions obtenues par la position des contours sont sur la surface du capteur, en pixels. Il faut donc faire une projection sur le plan $\\left(x,y\\right)$ du repère du convoyeur. Les méthodes suivantes ont été utilisées pour faire les opérations permettant de faire la conversion de repère.\n",
    "</div>\n",
    "\n",
    "- `getTransformationMatrix()`: Retourne la matrice de transformation entre le repère de la caméra et le repère du convoyeur ;\n",
    "- `getInverseCameraMatrix()`: Calcule l'inverse de la matrice de caméra en fonction des propriétés intrinsèques ;\n",
    "- `getItemsWorldCoordinates()`: Retourne la position dans le repère du convoyeur à partir d'une position dans le repère de la caméra, en pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### TRANSFORM METHODS                          ###\n",
    "##################################################\n",
    "            \n",
    "def getTransformationMatrix():\n",
    "    return np.array(([1,  0,  0, 0.500],\n",
    "                     [0, -1,  0, 0.200],\n",
    "                     [0,  0, -1, 0.282],\n",
    "                     [0,  0,  0,     1]))\n",
    "            \n",
    "def getInverseCameraMatrix(transformation_matrix):\n",
    "    sensor_pixel_width  = 640.0  # px\n",
    "    sensor_pixel_height = 427.0  # px\n",
    "    sensor_width        = 0.0234 # m\n",
    "    sensor_height       = 0.0156 # m\n",
    "    focal_length        = 0.0230 # m\n",
    "\n",
    "    fx = focal_length / sensor_width  * sensor_pixel_width  # px\n",
    "    fy = focal_length / sensor_height * sensor_pixel_height # px\n",
    "\n",
    "    # Set calibration matrix.\n",
    "    K = np.array([[fx,  0,  sensor_pixel_width/2],\n",
    "                  [0,  fy, sensor_pixel_height/2],\n",
    "                  [0,   0,                     1]])\n",
    "    \n",
    "    Kt = np.zeros((4,4))\n",
    "    Kt[:-1,:-1] = K\n",
    "    Kt[-1,-1]   = 1\n",
    "    \n",
    "    # Get camera matrix.\n",
    "    Pt = Kt @ np.linalg.inv(transformation_matrix)\n",
    "    \n",
    "    return np.linalg.inv(Pt)\n",
    "            \n",
    "def getItemsWorldCoordinates(items, transformation_matrix, inverse_camera_matrix):\n",
    "    for i, item in enumerate(items):\n",
    "        pos_x = item[\"x\"]\n",
    "        pos_y = item[\"y\"]\n",
    "        \n",
    "        # Transform screen coordinates into global position.\n",
    "        coordinates_screen      = np.array([pos_x, pos_y, 1, 1/transformation_matrix[2][-1]])\n",
    "        coordinates_reprojected = coordinates_screen / coordinates_screen[-1]\n",
    "        coordinates_world       = inverse_camera_matrix.dot(coordinates_reprojected) \n",
    "        \n",
    "        # Change item definition and content for export.\n",
    "        items[i] = [item[\"type\"], coordinates_world[0], coordinates_world[1], coordinates_world[2], item[\"angle\"]]\n",
    "        \n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Déverminage\n",
    "\n",
    "Lorsque le mode de déverminage est activé, on peut afficher et exporter les étapes du traitement d'images du pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### DEBUGGING METHODS                          ###\n",
    "##################################################\n",
    "\n",
    "def displayPipeline(original, final, pipeline, index):\n",
    "    simfig, plots = plt.subplots(4, 2, figsize = (6,10))\n",
    "        \n",
    "    plots[0][0].imshow(original)\n",
    "    plots[0][0].axis(\"off\")\n",
    "    plots[0][0].set_title(\"1. Original\")\n",
    "\n",
    "    plots[0][1].imshow(pipeline[0], cmap=\"gray\")\n",
    "    plots[0][1].axis(\"off\")\n",
    "    plots[0][1].set_title(\"2. Monochrome\")\n",
    "\n",
    "    plots[1][0].imshow(pipeline[1], cmap=\"gray\")\n",
    "    plots[1][0].axis(\"off\")\n",
    "    plots[1][0].set_title(\"3. Filtre gaussien\")\n",
    "\n",
    "    plots[1][1].imshow(pipeline[2], cmap=\"gray\")\n",
    "    plots[1][1].axis(\"off\")\n",
    "    plots[1][1].set_title(\"4. Seuillage\")\n",
    "\n",
    "    plots[2][0].imshow(pipeline[3], cmap=\"gray\")\n",
    "    plots[2][0].axis(\"off\")\n",
    "    plots[2][0].set_title(\"5. Dilatation\")\n",
    "\n",
    "    plots[2][1].imshow(pipeline[4], cmap=\"gray\")\n",
    "    plots[2][1].axis(\"off\")\n",
    "    plots[2][1].set_title(\"6. Canny\")\n",
    "\n",
    "    plots[3][0].imshow(pipeline[5], cmap=\"gray\")\n",
    "    plots[3][0].axis(\"off\")\n",
    "    plots[3][0].set_title(\"7. Cadre de délimitation\")\n",
    "\n",
    "    plots[3][1].imshow(final)\n",
    "    plots[3][1].axis(\"off\")\n",
    "    plots[3][1].set_title(\"8. Final\")\n",
    "\n",
    "    simfig.tight_layout()\n",
    "    simfig.savefig(f\"output/pipeline_{index+1:02}.pdf\", dpi=300)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Préambule du programme principal\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Cette section effectue l'initialisation des images, crée les répertoires nécessaires et vérifie la présences des fichiers requis pour faire le traitement. C'est à la deuxième ligne que l'on peut activer le mode de déverminage en passant le booléen correspondant <code>DEBUG</code> de <code>False</code> à <code>True</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to \"True\" if you wish to see the pipeline output step by step.\n",
    "DEBUG = False\n",
    "\n",
    "# Create output directory for CSV files (and debug files).\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.makedirs(\"./output\")\n",
    "\n",
    "# Verify that source files are available.\n",
    "images = os.listdir(\"photos_prob/\")\n",
    "if (len(images) == 0):\n",
    "    raise Exception(\"ERREUR! Vérifiez que vous avez bien un dossier photos_prob au même endroit que ce calepin.\")\n",
    "\n",
    "# Replace image path with image content.\n",
    "for i, f in enumerate(images):\n",
    "    img = cv2.imread(os.path.join(\"photos_prob/\", f))\n",
    "    images[i] = img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Programme principal\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Cette section du code appelle de façon séquentielle les fonctions permettant de faire la détection, la classification et la localisation des vis dans les images d'entrée. La détection des contours s'effectue d'abord, suivi par la transformation des positions du repère de la caméra jusqu'au repère du convoyeur. Les informations sont ensuite exportées dans un fichier CSV pour chaque image. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### MAIN PROGRAM                               ###\n",
    "##################################################\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    # Use processing pipeline to get contours.\n",
    "    contours, _, pipeline = getImageContours(image, get_pipeline=DEBUG)\n",
    "    \n",
    "    # Find valid contours and sort items.\n",
    "    items = sortImageContours(contours)\n",
    "    \n",
    "    # If debugging to get pipeline, make copy or original image and append last pipeline step before drawing.\n",
    "    if DEBUG:\n",
    "        image_original = image.copy()\n",
    "        pipeline.append(pipeline[-1].copy())\n",
    "    \n",
    "    # Draw box and add identification number as text.\n",
    "    drawImageContours(image, items, pipeline=pipeline)\n",
    "\n",
    "    # We wish to find the screws positions projected onto the frame {0} (conveyor) from the frame {1} (camera).\n",
    "    # The last function, \"getItemsWorldCoordinates\" will change items for dictionaries to lists before exporting.\n",
    "    transformation_matrix = getTransformationMatrix()\n",
    "    inverse_camera_matrix = getInverseCameraMatrix(transformation_matrix)\n",
    "    items                 = getItemsWorldCoordinates(items, transformation_matrix, inverse_camera_matrix)\n",
    "        \n",
    "    # Data export (CSV).\n",
    "    item_info = pd.DataFrame(items, columns = [\"Type\", \"X (m)\", \"Y (m)\", \"Z (m)\", u\"\\u03B8\"+\" (rad)\"])\n",
    "    item_info.index.name = \"id\"\n",
    "    item_info.to_csv(f\"output/image_{i+1:02}.csv\", float_format=\"%.3f\", encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # Show pipeline and save as PDF.\n",
    "    if DEBUG:\n",
    "        displayPipeline(image_original, image, pipeline, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse du résultat\n",
    "\n",
    "<br />\n",
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Pour valider que la chaîne de traitement d'images permettre de détecter, classer et localiser les vis dans les images d'entrée, il est important de valider visuellement que les étapes de traitements semblent produire les résultats attendus. Dans le code produit, il est possible d'activer le mode de déverminage (<code>DEBUG</code>) qui permet l'affichage et l'exportation complète d'images permettant de visualiser les étapes du traitement d'images. Le succès du système de vision pour détecter, classer et localiser les vis dans les images fournies a donc pu être validé et les résultats semblent démontrer que le système de vision permettrait d'accomplir le but recherché. Il est possible de voir ci-bas un example de chaque étape du traitement d'images pour bien visualiser leur importance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<div id=\"pipeline-example\" align=\"center\">\n",
    "    <img src=\"./input/pipeline_example_cabg2101_royo2206.svg\" alt=\"Exemple de la chaine de traitement d'images\" width=\"100%\"/>\n",
    "    <h2> Figure 2 ― Exemple de la chaîne de traitement d'images. </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; text-indent: 50px\">\n",
    "Dans la figure 2, il est possible de voir que le seuillage cause en effet les vis à s'amincir considérablement et même dans certain cas à les séparer en deux morceaux (comme dans le cas de la vis #9). C'est alors que l'on peut constater l'utilité de la transformation en dilatation pour assurer que chaque vis est reconnue comme étant une seule entité. L'algorithme de Canny de détection de bordures est capable de représenter de façon assez fidèle la forme de contour de chacune des vis. On peut finalement voir sur les deux dernières images que les rectangles de surface minimale permettent de localiser les vis mais aussi d'en déterminer la longueur et l'orientation.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "prob.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
